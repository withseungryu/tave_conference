{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alstm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\soynlp\\tokenizer\\_tokenizer.py:19: FutureWarning: Possible nested set at position 13\n",
      "  ('english & latin', re.compile(u\"[a-zA-ZÀ-ÿ]+[[`']?s]*|[a-zA-ZÀ-ÿ]+\", re.UNICODE))\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WordCloud' from 'wordcloud' (C:\\Users\\alstm\\dev\\tave\\wordcloud.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c684fcafe4ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\dev\\tave\\wordcloud.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'WordCloud' from 'wordcloud' (C:\\Users\\alstm\\dev\\tave\\wordcloud.py)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def remove_tag(content):\n",
    "   cleanr =re.compile('<.*?>')\n",
    "   cleantext = re.sub(cleanr, '', content)\n",
    "   return cleantext\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "remove_tag('<h3 class=\"petitionsView_title\">세종대왕 탄신일(5월15일)을 국가기념일로 지정해 주셨으면 합니다.</h3>')\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "pe_article_id=[]\n",
    "pe_start = []\n",
    "pe_end = []\n",
    "pe_votes = []\n",
    "pe_categorys = []\n",
    "pe_titles = []\n",
    "pe_contents = []\n",
    "#pe_article_id.append('article_id')\n",
    "#pe_start.append('start')\n",
    "#pe_end.append('end')\n",
    "#pe_votes.append('vote')\n",
    "#pe_titles.append('title')\n",
    "#pe_contents.append('content')\n",
    "for i in range(579901, 580000): \n",
    "    URL = \"https://www1.president.go.kr/petitions/\"+str(i)\n",
    "    response = requests.get(URL)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    titles = soup.find_all(\"h3\",{\"class\" :\"petitionsView_title\"})\n",
    "    if(len(titles)==0):\n",
    "        continue\n",
    "    votes = soup.find_all(\"span\", {\"class\" : \"counter\"})\n",
    "    contents = soup.find_all(\"div\", {\"class\" : \"View_write\"})\n",
    "    list_tag=[]\n",
    "    for ultag in soup.find_all('ul', {'class': 'petitionsView_info_list'}):\n",
    "        for litag in ultag.find_all('li'):\n",
    "            list_tag.append(litag.text)\n",
    "\n",
    "    pe_article_id.append(i)\n",
    "    pe_categorys.append(list_tag[0][4:])\n",
    "    pe_start.append(list_tag[1][4:])\n",
    "    pe_end.append(list_tag[2][4:])\n",
    "    pe_titles.append(titles[0].string)\n",
    "    pe_contents.append(contents[0].get_text(\" \", strip=True))\n",
    "    pe_votes.append(votes[0].string)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# df 으로 만들기\n",
    "df=0\n",
    "df=DataFrame()\n",
    "df[\"article_id\"]=pe_article_id\n",
    "df[\"start\"]=pe_start\n",
    "df[\"end\"]=pe_end\n",
    "df[\"answered\"]=0\n",
    "df[\"votes\"]=pe_votes\n",
    "df[\"category\"]=pe_categorys\n",
    "df[\"title\"]=pe_titles\n",
    "df[\"content\"]=pe_contents\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "category = pd.DataFrame(df['category'].value_counts()).reset_index()\n",
    "category.columns = ['category', 'counts']\n",
    "category\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "before1= df[df['category'].str.match('정치개혁')]\n",
    "before2 = df[df['category'].str.match('외교/통일/국방')]\n",
    "before3 = df[df['category'].str.match('일자리')]\n",
    "before4 = df[df['category'].str.match('보건복지')]\n",
    "before5 = df[df['category'].str.match('육아/교육')]\n",
    "before6 = df[df['category'].str.match('일자리')]\n",
    "before7 = df[df['category'].str.match('저출산/고령화대책')]\n",
    "before8 = df[df['category'].str.match('행정')]\n",
    "before9 = df[df['category'].str.match('반려동물')]\n",
    "before10 = df[df['category'].str.match('교통/건축/국토')]\n",
    "before11 = df[df['category'].str.match('경제민주화')]\n",
    "before12 = df[df['category'].str.match('인권/성평등')]\n",
    "before13 = df[df['category'].str.match('문화/예술/체육/언론')]\n",
    "before14 = df[df['category'].str.match('기타')]\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# df 저장\n",
    "df.to_csv(\"petition_word.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# df 가져오기\n",
    "df = pd.read_csv('petition_word.csv' )\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "df=df.dropna()\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "df.isnull().values.any()\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "from soynlp.tokenizer import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    # 개행문자 제거\n",
    "    text = re.sub('\\\\\\\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "# %time을 찍어주면 해당 코드를 실행할 때 걸리는 시간을 출력해 줍니다\n",
    "sentences1 = before1['content'].apply(preprocessing)\n",
    "sentences2 = before2['content'].apply(preprocessing)\n",
    "sentences3 = before3['content'].apply(preprocessing)\n",
    "sentences4 = before4['content'].apply(preprocessing)\n",
    "sentences5 = before5['content'].apply(preprocessing)\n",
    "sentences6 = before6['content'].apply(preprocessing)\n",
    "sentences7 = before7['content'].apply(preprocessing)\n",
    "sentences8 = before8['content'].apply(preprocessing)\n",
    "sentences9 = before9['content'].apply(preprocessing)\n",
    "sentences10 = before10['content'].apply(preprocessing)\n",
    "sentences11 = before11['content'].apply(preprocessing)\n",
    "sentences12 = before12['content'].apply(preprocessing)\n",
    "sentences13 = before13['content'].apply(preprocessing)\n",
    "sentences14 = before14['content'].apply(preprocessing)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "tokens1 = sentences1.apply(tokenizer.tokenize)\n",
    "tokens2 = sentences2.apply(tokenizer.tokenize)\n",
    "tokens3 = sentences3.apply(tokenizer.tokenize)\n",
    "tokens4 = sentences4.apply(tokenizer.tokenize)\n",
    "tokens5 = sentences5.apply(tokenizer.tokenize)\n",
    "tokens6 = sentences6.apply(tokenizer.tokenize)\n",
    "tokens7 = sentences7.apply(tokenizer.tokenize)\n",
    "tokens8 = sentences8.apply(tokenizer.tokenize)\n",
    "tokens9 = sentences9.apply(tokenizer.tokenize)\n",
    "tokens10 = sentences10.apply(tokenizer.tokenize)\n",
    "tokens11 = sentences11.apply(tokenizer.tokenize)\n",
    "tokens12 = sentences12.apply(tokenizer.tokenize)\n",
    "tokens13 = sentences13.apply(tokenizer.tokenize)\n",
    "tokens14 = sentences14.apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 나눔고딕 설치\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "stopwords_kr = ['하지만', '그리고', '그런데', '저는','제가',\n",
    "                '그럼', '이런', '저런', '합니다',\n",
    "                '많은', '많이', '정말', '너무'] \n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "def displayWordCloud(data = None, num=0, backgroundcolor = 'white', width=800, height=600):\n",
    "    wordcloud = WordCloud(\n",
    "                        font_path = fontpath, \n",
    "                        stopwords = stopwords_kr, \n",
    "                        background_color = backgroundcolor, \n",
    "                         width = width, height = height).generate(data)\n",
    "    wordcloud.to_file(os.path.join(currdir,\"wc\"+num+\".png\"))\n",
    "    #plt.figure(figsize = (15 , 10))\n",
    "    #plt.imshow(wordcloud)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show() \n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "from soynlp.noun import LRNounExtractor\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "\n",
    "noun_extractor = LRNounExtractor(verbose=True)\n",
    "noun_extractor.train(sentences1)\n",
    "nouns1 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences2)\n",
    "nouns2 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences3)\n",
    "nouns3 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences4)\n",
    "nouns4 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences5)\n",
    "nouns5 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences6)\n",
    "nouns6 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences7)\n",
    "nouns7 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences8)\n",
    "nouns8 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences9)\n",
    "nouns9 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences10)\n",
    "nouns10 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences11)\n",
    "nouns11 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences12)\n",
    "nouns12 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences13)\n",
    "nouns13 = noun_extractor.extract()\n",
    "noun_extractor.train(sentences14)\n",
    "nouns14 = noun_extractor.extract()\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# 추출된 명사를 찍어봅니다.\n",
    "displayWordCloud(' '.join(nouns1),1)\n",
    "displayWordCloud(' '.join(nouns2),2)\n",
    "displayWordCloud(' '.join(nouns3),3)\n",
    "displayWordCloud(' '.join(nouns4),4)\n",
    "displayWordCloud(' '.join(nouns5),5)\n",
    "displayWordCloud(' '.join(nouns6),6)\n",
    "displayWordCloud(' '.join(nouns7),7)\n",
    "displayWordCloud(' '.join(nouns8),8)\n",
    "displayWordCloud(' '.join(nouns9),9)\n",
    "displayWordCloud(' '.join(nouns10),10)\n",
    "displayWordCloud(' '.join(nouns11),11)\n",
    "displayWordCloud(' '.join(nouns12),12)\n",
    "displayWordCloud(' '.join(nouns13),13)\n",
    "displayWordCloud(' '.join(nouns14),14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
